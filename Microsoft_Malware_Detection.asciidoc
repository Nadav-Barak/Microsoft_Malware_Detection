= Microsoft Detection Classification

Our goal is to fit a model that can tell us according to some features
weather the device is detected or not. This work was done it
colabaration with Dror Salti


+*In[1]:*+
[source, ipython3]
----
from scipy import io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
import random
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import lightgbm as lgb
import re
from sklearn.neighbors import KNeighborsClassifier
%matplotlib inline
----

= Loading The Data

The data can be downloaded from Kaggle in the following link:

https://www.kaggle.com/c/microsoft-malware-prediction/data

Due to the large scale of the dataset (8.6 million records), we analyzed
the structure of the data, applied different preprocess tools and
excecuted the differents models hyper parameter tuning on a portion of
the data (10%) that was randomly selected. Afterwards we redid the
process with the full data before modeling.


+*In[2]:*+
[source, ipython3]
----
path = 'C:/Users/admin/Desktop/לימוד אינטרנטי/microsoft_malware/train.csv'

#p = 0.1  # 10% of the lines
#dataset = pd.read_csv( path, header=0, skiprows=lambda i: i>0 and random.random() > p)

dataset = pd.read_csv(path)
pd.set_option("display.precision", 8)
pd.set_option("display.expand_frame_repr", False)
pd.set_option("display.max_rows", 100)
----


+*Out[2]:*+
----
C:\Users\admin\Documents\anaconda\lib\site-packages\IPython\core\interactiveshell.py:3063: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.
  interactivity=interactivity, compiler=compiler, result=result)
----

= Preprocessing the data

First of all we look on the ``dry'' details of each variable: 1. Missing
values fraction 2. Skewness measure 3. Number of unique values of each
variable


+*In[3]:*+
[source, ipython3]
----
print((dataset.isnull().sum()/dataset.shape[0]).sort_values(ascending=False))

----


+*Out[3]:*+
----
PuaMode                                              0.99973925
Census_ProcessorClass                                0.99589925
DefaultBrowsersIdentifier                            0.95137750
Census_IsFlightingInternal                           0.83037975
Census_InternalBatteryType                           0.71026725
Census_ThresholdOptIn                                0.63499075
Census_IsWIMBootEnabled                              0.63414000
SmartScreen                                          0.35621575
OrganizationIdentifier                               0.30839350
SMode                                                0.06025950
CityIdentifier                                       0.03654600
Wdft_IsGamer                                         0.03397350
Wdft_RegionIdentifier                                0.03397350
Census_InternalBatteryNumberOfCharges                0.03012975
Census_FirmwareManufacturerIdentifier                0.02047600
Census_IsFlightsDisabled                             0.01796975
Census_FirmwareVersionIdentifier                     0.01789425
Census_OEMModelIdentifier                            0.01146300
Census_OEMNameIdentifier                             0.01067500
Firewall                                             0.01025200
Census_TotalPhysicalRAM                              0.00901725
Census_IsAlwaysOnAlwaysConnectedCapable              0.00798700
Census_OSInstallLanguageIdentifier                   0.00674500
IeVerIdentifier                                      0.00665525
Census_PrimaryDiskTotalCapacity                      0.00596275
Census_SystemVolumeTotalCapacity                     0.00596100
Census_InternalPrimaryDiagonalDisplaySizeInInches    0.00529025
Census_InternalPrimaryDisplayResolutionHorizontal    0.00527225
Census_InternalPrimaryDisplayResolutionVertical      0.00527225
Census_ProcessorModelIdentifier                      0.00466800
Census_ProcessorManufacturerIdentifier               0.00466475
Census_ProcessorCoreCount                            0.00466425
AVProductsEnabled                                    0.00404625
AVProductsInstalled                                  0.00404625
AVProductStatesIdentifier                            0.00404625
IsProtected                                          0.00402625
RtpStateBitfield                                     0.00361150
Census_IsVirtualDevice                               0.00178900
Census_PrimaryDiskTypeName                           0.00144425
UacLuaenable                                         0.00119650
Census_ChassisTypeName                               0.00006825
GeoNameIdentifier                                    0.00002150
Census_PowerPlatformRoleName                         0.00000525
OsBuildLab                                           0.00000175
LocaleEnglishNameIdentifier                          0.00000000
AvSigVersion                                         0.00000000
OsPlatformSubRelease                                 0.00000000
Processor                                            0.00000000
OsVer                                                0.00000000
AppVersion                                           0.00000000
Platform                                             0.00000000
EngineVersion                                        0.00000000
CountryIdentifier                                    0.00000000
ProductName                                          0.00000000
HasTpm                                               0.00000000
OsBuild                                              0.00000000
IsBeta                                               0.00000000
OsSuite                                              0.00000000
IsSxsPassiveMode                                     0.00000000
HasDetections                                        0.00000000
SkuEdition                                           0.00000000
Census_OSInstallTypeName                             0.00000000
Census_IsPenCapable                                  0.00000000
Census_IsTouchEnabled                                0.00000000
Census_IsSecureBootEnabled                           0.00000000
Census_FlightRing                                    0.00000000
Census_ActivationChannel                             0.00000000
Census_GenuineStateName                              0.00000000
Census_IsPortableOperatingSystem                     0.00000000
Census_OSWUAutoUpdateOptionsName                     0.00000000
Census_OSUILocaleIdentifier                          0.00000000
Census_OSSkuName                                     0.00000000
AutoSampleOptIn                                      0.00000000
Census_OSEdition                                     0.00000000
Census_OSBuildRevision                               0.00000000
Census_OSBuildNumber                                 0.00000000
Census_OSBranch                                      0.00000000
Census_OSArchitecture                                0.00000000
Census_OSVersion                                     0.00000000
Census_HasOpticalDiskDrive                           0.00000000
Census_DeviceFamily                                  0.00000000
Census_MDC2FormFactor                                0.00000000
MachineIdentifier                                    0.00000000
dtype: float64
----

As we can see, there are some featurs that is not informative: too many
NULL’s, too many categories, or skewness very high (actually there is
one category).

== Dealing with NA Values

*We decided to remove the following features because the NA fraction was
too large:*

`PuaMode' (99.97%), `Census_ProcessorClass' (99.6%),
`Census_InternalBatteryType' (71%), `Census_IsFlightingInternal (83%)',
`Census_ThresholdOptIn' (63.5%), `Census_IsWIMBootEnabled' (63.4%),
`SmartScreen' (35.61%).

*For the following features we decided to remove the records with NA
values because the fraction of missingness is small (0.4%) and the same
records has NA in those variables:*

`AVProductStatesIdentifier', `AVProductsInstalled', `AVProductsEnabled',
`Census_ProcessorCoreCount'.

*For the following features we decided too fill NA with most common
value. We didn’t want to remove more records:*

`RtpStateBitfield', `Firewall', `Census_IsVirtualDevice',
`Census_IsAlwaysOnAlwaysConnectedCapable', `Wdft_IsGamer'.

*For the numerical variables with NA values we decided to fill these
value with the median. The features are:*

`Census_PrimaryDiskTotalCapacity',
`Census_SystemVolumeTotalCapacity',`Census_TotalPhysicalRAM',
`Census_InternalPrimaryDiagonalDisplaySizeInInches',
`Census_InternalPrimaryDisplayResolutionHorizontal',
`Census_InternalPrimaryDisplayResolutionVertical',
`Census_InternalBatteryNumberOfCharges'.

*One feature with many NA values is the deafultBrowser. We decided to
treat this feature as a binary variable- 1 if there is a deafult browser
and 0 other wise*

Later, we explain how we chose to deal with the other variables with NA
values.


+*In[4]:*+
[source, ipython3]
----
tooManyNULLVars = ['PuaMode', 'Census_ProcessorClass', 'Census_InternalBatteryType', 'Census_IsFlightingInternal',
                   'Census_ThresholdOptIn', 'Census_IsWIMBootEnabled'] #more than 50% missing values
#remove rows with NA
#vars = ['AVProductStatesIdentifier', 'AVProductsInstalled', 'AVProductsEnabled', 'Census_ProcessorCoreCount']
#for cat in vars:
    #dataset = dataset[~np.isnan(dataset[cat])]

#fill most common value in NA
vars = ['RtpStateBitfield', 'Firewall', 'Census_IsVirtualDevice', 'Census_IsAlwaysOnAlwaysConnectedCapable', 
        'Wdft_IsGamer','AVProductsInstalled','AVProductsEnabled','IsProtected']
for cat in vars:
    dataset[cat] = dataset[cat].fillna(dataset[cat].value_counts().index[0])

#fill with unique value
vars = ['SmartScreen']
dataset[vars] = dataset[vars].fillna("empty")

#fill median value in NA
vars = ['Census_PrimaryDiskTotalCapacity', 'Census_SystemVolumeTotalCapacity','Census_TotalPhysicalRAM',
        'Census_InternalPrimaryDiagonalDisplaySizeInInches', 'Census_InternalPrimaryDisplayResolutionHorizontal',
        'Census_InternalPrimaryDisplayResolutionVertical', 'Census_InternalBatteryNumberOfCharges','Census_ProcessorCoreCount']
for cat in vars:
    dataset[cat] = dataset[cat].fillna(dataset[cat].median())

#null or not null
vars = ['DefaultBrowsersIdentifier']
for cat in vars:
    dataset['DefaultBrowsersIdentifier'] = dataset['DefaultBrowsersIdentifier'].isnull().astype('int')

----

== Special Features

1.  We noticed that in `ProductName' feature, 98.5% are `win8defender',
1.49% are `mse' and 0.1% are niether both. So we change them to
`win8defender'.
2.  We noticed that in `Census_PrimaryDiskTypeName' feature, there are 2
major categories: `HDD' and `SSD' and the rest of the categories are
very small. So we united them to one category, named `other'.
3.  We noticed that in `Processor' and `Census_OSArchitecture' features
there are some categories that contains 64 so we united them to one
category- `64'.


+*In[5]:*+
[source, ipython3]
----
#column 1 - productName
#98.5% one major category, 1.45% minor category, 8 other so we change them to the main category
dataset['ProductName'] = dataset['ProductName'].apply(lambda x: x if x in ['mse'] else 'win8defender')

##column 43 - Census_PrimaryDiskTypeName
#2 major categories, the rest of the rows change to other categories
dataset['Census_PrimaryDiskTypeName'] = dataset['Census_PrimaryDiskTypeName'].apply(lambda x: x if x in ['HDD','SSD'] else 'other')

#columns 19,55 - Processor, Census_OSArchitecture
vars = ['Processor', 'Census_OSArchitecture']
for var in vars:
    dataset[var] = dataset[var].apply(lambda x: '64' if x in ['amd64', 'arm64', 'x64'] else '86')

----

== Categorial variables

There are many features that are categorial. Some of them with very
small categories, so we decided to unite these categories into one
-`other', and to add to this category the NA values.


+*In[6]:*+
[source, ipython3]
----
#categorical variables
categorial = ['ProductName','AVProductStatesIdentifier', 'CountryIdentifier', 'LocaleEnglishNameIdentifier', 'OsPlatformSubRelease',
              'OsSuite', 'SkuEdition', 'IeVerIdentifier', 'Census_MDC2FormFactor', 'Census_OEMNameIdentifier',
              'Census_ProcessorManufacturerIdentifier', 'Census_PrimaryDiskTypeName', 'Census_ChassisTypeName', 'Census_PowerPlatformRoleName',
              'Census_OSBranch', 'Census_OSBuildNumber', 'Census_OSEdition', 'Census_OSInstallTypeName', 'Census_OSInstallLanguageIdentifier',
              'Census_OSUILocaleIdentifier', 'Census_OSWUAutoUpdateOptionsName', 'Census_GenuineStateName', 'Census_ActivationChannel',
              'Census_FlightRing', 'Census_FirmwareManufacturerIdentifier', 'Wdft_RegionIdentifier', 'Processor', 'Census_OSArchitecture','SmartScreen']
for cat in categorial:
    dataset[cat] = dataset[cat].fillna(-1)
    valCounts = dataset[cat].value_counts()
    dataset[cat] = dataset[cat].apply(lambda x: x if valCounts[x] > 0.005 * len(dataset) else -1)
    dataset[cat] = dataset[cat].astype("category")
    
----

== Versions Features

We have in our data 3 versions features: `EngineVersion', `AppVersion',
`AvSigVersion'. We decided to rank them from old to new.


+*In[11]:*+
[source, ipython3]
----
def removeDots(string):
    ans = []
    temp = ""
    for s in string:
        if(s == '.'):
            ans.append(str(temp))
            temp=''
        else:
            temp = temp+ str(s)
    ans.append(str(temp))
    ans.append(string)
    return ans

def sortVersions(data):
    values = list(data.unique())
    df = pd.DataFrame(columns=["A", "B", "C", "D", "string"])
    for i in range(len(values)):
        x = removeDots(str(values[i]))
        df.loc[i] = x
    df = df.sort_values(by=['A', 'B', 'C', 'D'])
    return list(df['string'])
    
#ordinal version variables
versions = ['AvSigVersion','EngineVersion', 'AppVersion']
for cat in versions:
    orders = sortVersions(dataset[cat])
    dataset[cat] = dataset[cat].apply(lambda x: orders.index(x))

----

== Skewed Features


+*In[12]:*+
[source, ipython3]
----
pd.options.display.float_format = '{:,.4f}'.format
sk_df = pd.DataFrame([{'column': c, 'uniq': dataset[c].nunique(), 'skewness': dataset[c].value_counts(normalize=True).values[0] * 100, 'type': dataset[c].dtype}
                      for c in dataset.columns])
sk_df = sk_df.sort_values('skewness', ascending=False)
print(sk_df)
----


+*Out[12]:*+
----
                                               column     uniq  skewness      type
75                            Census_IsWIMBootEnabled        1  100.0000   float64
28                                            PuaMode        1  100.0000    object
5                                              IsBeta        2   99.9992     int64
69                           Census_IsFlightsDisabled        2   99.9991   float64
68                         Census_IsFlightingInternal        2   99.9981   float64
27                                    AutoSampleOptIn        2   99.9973     int64
71                              Census_ThresholdOptIn        2   99.9736   float64
29                                              SMode        2   99.9550   float64
65                   Census_IsPortableOperatingSystem        2   99.9437     int64
35                                Census_DeviceFamily        3   99.8383    object
83                                   OrganizationSize        3   99.7500  category
33                                       UacLuaenable        9   99.3899   float64
76                             Census_IsVirtualDevice        2   99.2971   float64
1                                         ProductName        2   98.9357  category
12                                             HasTpm        2   98.7955     int64
7                                    IsSxsPassiveMode        2   98.2688     int64
32                                           Firewall        2   97.8814   float64
11                                  AVProductsEnabled        6   97.3999   float64
6                                    RtpStateBitfield        7   97.3389   float64
20                                              OsVer       41   96.7570    object
18                                           Platform        4   96.6017    object
78                                Census_IsPenCapable        2   96.1970     int64
8                           DefaultBrowsersIdentifier        2   95.1377     int32
26                                        IsProtected        2   94.5866   float64
79            Census_IsAlwaysOnAlwaysConnectedCapable        2   94.2999   float64
70                                  Census_FlightRing        4   93.6684  category
45                         Census_HasOpticalDiskDrive        2   92.2889     int64
55                              Census_OSArchitecture        2   90.8498  category
19                                          Processor        2   90.8445  category
66                            Census_GenuineStateName        4   88.3109  category
39             Census_ProcessorManufacturerIdentifier        3   87.8633  category
77                              Census_IsTouchEnabled        2   87.4393     int64
52                         Census_InternalBatteryType       59   78.4780    object
80                                       Wdft_IsGamer        2   72.6131   float64
10                                AVProductsInstalled        7   69.9902   float64
51                       Census_PowerPlatformRoleName        5   69.3041  category
9                           AVProductStatesIdentifier       13   65.2779  category
43                         Census_PrimaryDiskTypeName        3   65.0729  category
34                              Census_MDC2FormFactor        8   64.1395  category
22                                            OsSuite        3   62.3163  category
25                                         SkuEdition        4   61.7968  category
38                          Census_ProcessorCoreCount       42   61.3474   float64
53              Census_InternalBatteryNumberOfCharges    25164   59.6632   float64
47                             Census_ChassisTypeName       13   58.8457  category
3                                          AppVersion      102   57.6078     int64
41                              Census_ProcessorClass        3   57.0749    object
50    Census_InternalPrimaryDisplayResolutionVertical     1248   56.2784   float64
67                           Census_ActivationChannel        5   52.9942  category
74                         Census_IsSecureBootEnabled        2   51.4024     int64
49  Census_InternalPrimaryDisplayResolutionHorizontal     1630   51.1424   float64
82                                      HasDetections        2   50.0449     int64
31                                        SmartScreen        6   48.3778  category
15                             OrganizationIdentifier        3   47.0453  category
46                            Census_TotalPhysicalRAM     2136   46.8006   float64
56                                    Census_OSBranch       10   44.9368  category
57                               Census_OSBuildNumber        7   44.9340  category
64                   Census_OSWUAutoUpdateOptionsName        5   44.3290  category
23                               OsPlatformSubRelease        9   43.8841  category
21                                            OsBuild       60   43.8840     int64
30                                    IeVerIdentifier       16   43.5460  category
2                                       EngineVersion       65   43.1028     int64
24                                         OsBuildLab      597   40.9971    object
59                                   Census_OSEdition        6   38.8996  category
60                                   Census_OSSkuName       26   38.8984    object
62                 Census_OSInstallLanguageIdentifier       23   35.6314  category
63                        Census_OSUILocaleIdentifier       23   35.5358  category
48  Census_InternalPrimaryDiagonalDisplaySizeInInches      703   34.6867   float64
42                    Census_PrimaryDiskTotalCapacity     3582   32.4415   float64
72              Census_FirmwareManufacturerIdentifier       19   30.2417  category
61                           Census_OSInstallTypeName        9   29.2477  category
17                        LocaleEnglishNameIdentifier       38   23.4759  category
13                                  CountryIdentifier       53   20.3386  category
81                              Wdft_RegionIdentifier       15   20.1927  category
16                                  GeoNameIdentifier      287   17.1731   float64
58                             Census_OSBuildRevision      263   15.8424     int64
54                                   Census_OSVersion      393   15.8424    object
36                           Census_OEMNameIdentifier       21   14.4421  category
37                          Census_OEMModelIdentifier   121547    3.4474   float64
40                    Census_ProcessorModelIdentifier     3049    3.2690   float64
4                                        AvSigVersion     8293    1.1518     int64
14                                     CityIdentifier    84997    1.1036   float64
73                   Census_FirmwareVersionIdentifier    41968    1.0272   float64
44                   Census_SystemVolumeTotalCapacity   407315    0.6447   float64
0                                   MachineIdentifier  4000000    0.0000    object
----

The following features has high skewnees: `IsBeta', `AutoSampleOptIn',
`SMode', `Census_DeviceFamily', `Census_IsPortableOperatingSystem',
`Census_IsFlightsDisabled'.

Actually more then 99.9% of the data are in the same category in those
features. Therefore we decided to remove them (in order to avoid
over-fitting).


+*In[13]:*+
[source, ipython3]
----
oneCategoryVars = ['IsBeta', 'AutoSampleOptIn', 'SMode', 'Census_DeviceFamily',
                   'Census_IsPortableOperatingSystem', 'Census_IsFlightsDisabled','UacLuaenable'] #only one category in the data (or 99% of it)
----

== More uninformative features

We found out that are some features that are contained in others.
Moreover we noticed that there are some categorial features with too
many categories. This situation can lead to an over fitting problem and
to increase the runtime of the process (creating many dummies
variables).

So we decided to remove them.


+*In[14]:*+
[source, ipython3]
----
unInformativeVars= ['CityIdentifier', 'GeoNameIdentifier', 'Census_OEMModelIdentifier',
                    'Census_ProcessorModelIdentifier', 'Census_OSBuildRevision', 'Census_FirmwareVersionIdentifier'] # too many categories
containedVars = ['OsVer', 'Platform', 'OsBuildLab', 'Census_OSVersion', 'Census_OSSkuName'] #data that can be inferred from other variables
----


+*In[15]:*+
[source, ipython3]
----
delVars = tooManyNULLVars + unInformativeVars + containedVars + oneCategoryVars
dataset.drop(delVars, axis=1, inplace=True)
----

We created a new feature `Organization Size'. This feature describe the
size of the organization that the device belongs to. We decided to
divide the organizations into three categories: 1. Big - more than 10000
devices 2. Medium - between 100-10000 devices. 3. Small - less than 100
devices.

In addition we noticed that `OrganizationIdentifier' contains 2 major
categories ``27'' and ``18'' and many more categories that are much
smaller. So we united them into one category.


+*In[16]:*+
[source, ipython3]
----
#-----creating new features----
x = dataset['OrganizationIdentifier'].value_counts()
new = x[dataset['OrganizationIdentifier']]
new[np.isnan(new)] = 0
new = np.asarray(new)
dataset['OrganizationSize'] = new
dataset['OrganizationSize'] = pd.cut(dataset['OrganizationSize'].rank(method='first'),
                                     bins=[0,100,10000,10000000], labels=['small', 'medium', 'big'])
dataset['OrganizationSize'] = dataset['OrganizationSize'].astype('category')
dataset['OrganizationIdentifier'] = dataset['OrganizationIdentifier'].apply(lambda x: x if x in [18,27] else 0)
dataset['OrganizationIdentifier'] = dataset['OrganizationIdentifier'].astype('category')
----

== Correlation Matrix

We would like to plot the correlation matrix of the numerical variables
of the data in order to diagnose high correlated variables.


+*In[17]:*+
[source, ipython3]
----
#----Plots----
# calculate the correlation matrix
corr = dataset.corr()
plt.figure(figsize=(15,15))
sns.heatmap(corr, cmap='RdBu_r', annot=True, center=0.0)
plt.title('Correlation between Numeric Variables')
sns.despine()
----


+*Out[17]:*+
----
![png](output_25_0.png)
----

As we can see there are 3 pairs of variables that are highly correlated
(absolute value greater than 0.85). So we decided to remove one of them
that has lower correlation with the target value. We chose not to remove
Engine_Version because it has a relatively high correlation with the
target variable.

So we remove `IsSxsPassiveMode',
`Census_InternalPrimaryDisplayResolutionVertical'


+*In[18]:*+
[source, ipython3]
----
highCorrVars = ['IsSxsPassiveMode', 'Census_InternalPrimaryDisplayResolutionVertical']
dataset.drop(highCorrVars, axis=1, inplace=True)
----

=== Prepearing the Data for the algorithms


+*In[19]:*+
[source, ipython3]
----
y = dataset['HasDetections']
MachineIdentifier = dataset['MachineIdentifier']
dataset.drop(['MachineIdentifier','HasDetections'], axis=1, inplace=True)

Categories = categorial + ['OrganizationIdentifier', 'OrganizationSize']
train = pd.get_dummies(dataset, columns=Categories)
train = train.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', 'Dot', x))
#spliting the data to train and validation sets
x_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.20, random_state=42)
del train, dataset
x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.25, random_state=42)

----

= Machine Learning Models

We are fitting some ML models to the data: Random Forest, XGBoost, SVM,
KNN.


+*In[ ]:*+
[source, ipython3]
----
from collections import Counter
z = list(x_train.columns[192:])
print(Counter(z))

----

== Random Forest

The first model that we tried to fit is random forest.

In the first stage we are tunning the hyperparameters of the models. We
chose to tune the following parameters wuth grid search: 1. Number of
trees. 2. Max depth of each tree. 3. The criterion of building the tree.


+*In[ ]:*+
[source, ipython3]
----
#hyperparameters for random forest - grid search
params_grid = [{'criterion': ['gini', 'entropy'], 'max_depth': [10, 25, 50], 'n_estimators': [50, 100, 300]}]
rf_model = GridSearchCV(RandomForestClassifier(), params_grid, scoring='accuracy', cv=3, n_jobs=-1, verbose=3)
rf_model.fit(x_train, y_train)

print('\n Best hyperparameters:')
print(rf_model.best_params_)

----

The parameters that we got are: 1. Number of trees - 100 2. Max depth -
25 3. Criterion - entropy

Now we train the model again (this time on all the train set) and see
the accuracy rate:


+*In[22]:*+
[source, ipython3]
----
RF_MODEL = RandomForestClassifier(criterion='entropy', max_depth=25, n_estimators=100, verbose=1, n_jobs=-1)
RF_MODEL.fit(x_train, np.ravel(y_train))
----


+*Out[22]:*+
----
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.1min
[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  6.5min finished
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='entropy', max_depth=25, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=-1, oob_score=False, random_state=None, verbose=1,
                       warm_start=False)----


+*In[23]:*+
[source, ipython3]
----
y_predTrain = RF_MODEL.predict(x_train)
accTrain = accuracy_score(y_train, y_predTrain)
print("Accuarcy on train set is: " + str(accTrain))

y_pred_RF = RF_MODEL.predict(x_cv)
acc = accuracy_score(y_cv, y_pred_RF)
print("Accuarcy on validation set is: " + str(acc))

confusion = confusion_matrix(y_cv, y_pred_RF)
df_cm = pd.DataFrame(confusion, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (5,5))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16}, fmt='g')# font size
sns.despine()
----


+*Out[23]:*+
----
[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    7.5s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:   23.6s finished

Accuarcy on train set is: 0.7412883333333333

[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.
[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    2.3s
[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    7.6s finished

Accuarcy on validation set is: 0.65305125

![png](output_36_4.png)
----

The Random-Forest model achived accuracy on the validation set.

== XGBoost

In the first stage we are tunning the hyperparameters of the models. We
chose to tune the following parameters wuth grid search:

1.  max_depth
2.  subsample
3.  gamma
4.  colsample_bytree


+*In[ ]:*+
[source, ipython3]
----
params = {"max_depth"        : [ 15, 25, 35],
 "subsample" : [ 0.7, 0.8, 0.9, 1],
 "gamma"            : [ 0.1, , 1, 5 ],
 "colsample_bytree" : [ 0.3,  0.5 , 0.8 ] }

xgb = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic')

xgb_model = GridSearchCV(xgb, params_grid, cv=5, n_jobs=-1,verbose=2, scoring='accuracy')
xgb_model.fit(x_train, y_train)

print('\n Best hyperparameters:')
print(xgb_model.best_params_)
----

The parameters that we chose according to the grid search are: 1.
colsample_bytree = 0.4. 2. subsample = 0.8. 3. max depth = 15. 4. gamma
= 1.

Now we train the model again (this time on all the train set) and see
the accuracy rate:


+*In[20]:*+
[source, ipython3]
----
xgb_model = xgb.XGBClassifier(scale_pos_weight=1, learning_rate=0.02, verbosity=1, 
                      colsample_bytree = 0.8, subsample = 0.8, objective='binary:logistic',
                      n_estimators=100, reg_alpha = 0.3, max_depth=15, gamma=1, n_jobs=-1)
xgb_model.fit(x_train, np.ravel(y_train))
----


+*Out[20]:*+
----XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.8, gamma=1, gpu_id=-1,
              importance_type='gain', interaction_constraints='',
              learning_rate=0.02, max_delta_step=0, max_depth=15,
              min_child_weight=1, missing=nan, monotone_constraints='()',
              n_estimators=100, n_jobs=-1, num_parallel_tree=1,
              objective='binary:logistic', random_state=0, reg_alpha=0.3,
              reg_lambda=1, scale_pos_weight=1, subsample=0.8,
              tree_method='exact', validate_parameters=1, verbosity=1)----


+*In[21]:*+
[source, ipython3]
----
y_predTrain = xgb_model.predict(x_train)
accTrain = accuracy_score(y_train, y_predTrain)
print("Accuarcy on train set is: " + str(accTrain))

y_pred_xgb = xgb_model.predict(x_cv)
acc = accuracy_score(y_cv, y_pred_xgb)
print("Accuarcy is: " + str(acc))

confusion = confusion_matrix(y_cv, y_pred_xgb)
df_cm = pd.DataFrame(confusion, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (5,5))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16}, fmt='g')# font size
sns.despine()
----


+*Out[21]:*+
----
Accuarcy on train set is: 0.6800975
Accuarcy is: 0.65484

![png](output_42_1.png)
----

The XGBoost model achived accuracy on the validation set.

== LGBM

We chose to tune the following hyperparameters: 1. boosting type. 2. num
of leaves. 3. max depth. 4. number of tree.

We chose relatively large values for max depth and number of leaves due
to the high dimensionality of the data-set.


+*In[ ]:*+
[source, ipython3]
----
lgbm = lgb.LGBMClassifier()
boosting_type = ['gbdt', 'dart']
num_leaves = [100,300,500,1000, 2000] #list(range(30, 150)),
max_depth = list(range(7,25,3))
n_estimators = [100, 250, 500, 1000, 2000]
lgbm_grid = dict(boosting_type=boosting_type, num_leaves=num_leaves,
                 max_depth = max_depth, n_estimators=n_estimators)


grid_search = GridSearchCV(estimator=lgbm, param_grid=lgbm_grid,
              n_jobs=-1, scoring='accuracy', error_score=0, verbose=2, cv=3)

print('\n Best hyperparameters:')
print(lgbm.best_params_)
----

The parameters that we got are: 1. boosting type: `gbdt' 2. num of
leaves: 1024 3. max depth: 13 4. number of tree: 1000

The values for the following hyperparameters we took from the XGboost
model in order to reduce the computation time: subsample,
colsample_bytree, reg_alpha.

Now we train the model again (this time on all the train set) and see
the accuracy rate:


+*In[24]:*+
[source, ipython3]
----
lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=800, max_depth=13,
                                verbosity=1, learning_rate=0.02, n_estimators=1000,
                                subsample=0.8, colsample_bytree=0.8, reg_alpha=0.3)
lgb_model.fit(x_train, np.ravel(y_train))
----


+*Out[24]:*+
----LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.8,
               importance_type='split', learning_rate=0.02, max_depth=13,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=1000, n_jobs=-1, num_leaves=800, objective=None,
               random_state=None, reg_alpha=0.3, reg_lambda=0.0, silent=True,
               subsample=0.8, subsample_for_bin=200000, subsample_freq=0,
               verbosity=1)----


+*In[25]:*+
[source, ipython3]
----
y_predTrain = lgb_model.predict(x_train)
accTrain = accuracy_score(y_train, y_predTrain)
print("Accuarcy on train set is: " + str(accTrain))

y_pred_lgbm = lgb_model.predict(x_cv)
acc = accuracy_score(y_cv, y_pred_lgbm)
print("Accuarcy is: " + str(acc))

confusion = confusion_matrix(y_cv, y_pred_lgbm)
df_cm = pd.DataFrame(confusion, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (5,5))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16}, fmt='g')# font size
sns.despine()
----


+*Out[25]:*+
----
Accuarcy on train set is: 0.6958525
Accuarcy is: 0.66451125

![png](output_48_1.png)
----

The LGBM model achived accuracy on the validation set.

== Logistic Regression

We tried to fit the data to a logistic regression model, in order to get
more information about the relevant features to the target value


+*In[28]:*+
[source, ipython3]
----
import statsmodels.api as sm
----


+*In[ ]:*+
[source, ipython3]
----
LR_model = sm.Logit(y_train, x_train)
LR_res = LR_model.fit()
print(LR_res.summary())
----

As we can see the latexmath:[$R^2$] measure is very low (0.04), which
means that this model is not fit to this data and therefore we won’t use
it.

= Ensembling Models

For the second stage of building our classifier, we decided to ensemble
the 3 models: Random-Forest, XGBoost and LightGBM, in order to reduce
the over-fitting and achive high accuracy.

We tried 2 different ensembling methods: 1. Logistic regression model 2.
KNN

First, we plot the corrlation matrix between the predictions of the 3
pre-trained models:


+*In[26]:*+
[source, ipython3]
----
preds = pd.DataFrame(
    {'RandomForest': y_pred_RF,
     'XGBoost': y_pred_xgb,
     'LGBM': y_pred_lgbm
    })
corrPreds = preds.corr()
plt.figure(figsize=(5,5))
sns.heatmap(corrPreds, cmap='RdBu_r', annot=True, center=0.0)
plt.title('Correlation between predictions')
sns.despine()
----


+*Out[26]:*+
----
![png](output_56_0.png)
----

== Logistic Regression


+*In[30]:*+
[source, ipython3]
----
LR_model = sm.Logit(np.ravel(y_cv),sm.add_constant(preds))
LR_res = LR_model.fit()
print(LR_res.summary())

finalpreds = LR_res.predict(sm.add_constant(preds))
finalpreds = finalpreds.apply(lambda x: 1 if x > 0.5 else 0)
print("The Accuracy is: " + str(accuracy_score(finalpreds, y_cv)))

----


+*Out[30]:*+
----
Optimization terminated successfully.
         Current function value: 0.634209
         Iterations 4
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                      y   No. Observations:               800000
Model:                          Logit   Df Residuals:                   799996
Method:                           MLE   Df Model:                            3
Date:                Mon, 13 Jul 2020   Pseudo R-squ.:                 0.08503
Time:                        22:28:20   Log-Likelihood:            -5.0737e+05
converged:                       True   LL-Null:                   -5.5452e+05
Covariance Type:            nonrobust   LLR p-value:                     0.000
================================================================================
                   coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------
const           -0.7433      0.003   -214.396      0.000      -0.750      -0.737
RandomForest     0.4003      0.009     45.797      0.000       0.383       0.417
XGBoost          0.2672      0.010     27.682      0.000       0.248       0.286
LGBM             0.8424      0.008    100.532      0.000       0.826       0.859
================================================================================
The Accuracy is: 0.66451125
----


+*In[43]:*+
[source, ipython3]
----
# The resulting model provides the exact same prediction the lgbm model alone
----

== KNN


+*In[34]:*+
[source, ipython3]
----
#knn
k_range = [4,6,8,10,15]
score_list =[]
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(preds, np.ravel(y_cv))
    y_pred = knn.predict(preds)
    acc = accuracy_score(y_cv, y_pred)
    score_list.append(acc)
    print("The Score for k=" +str(k) + " is: " + str(acc))
plt.plot(k_range, score_list)
plt.show()
sns.despine()
----


+*Out[34]:*+
----
The Score for k=4 is: 0.50074875


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-34-e44f102a9cf2> in <module>
          5     knn = KNeighborsClassifier(n_neighbors=k)
          6     knn.fit(preds, np.ravel(y_cv))
    ----> 7     y_pred = knn.predict(preds)
          8     acc = accuracy_score(y_cv, y_pred)
          9     score_list.append(acc)
    

    ~\Documents\anaconda\lib\site-packages\sklearn\neighbors\_classification.py in predict(self, X)
        171         X = check_array(X, accept_sparse='csr')
        172 
    --> 173         neigh_dist, neigh_ind = self.kneighbors(X)
        174         classes_ = self.classes_
        175         _y = self._y
    

    ~\Documents\anaconda\lib\site-packages\sklearn\neighbors\_base.py in kneighbors(self, X, n_neighbors, return_distance)
        661                 delayed_query(
        662                     self._tree, X[s], n_neighbors, return_distance)
    --> 663                 for s in gen_even_slices(X.shape[0], n_jobs)
        664             )
        665         else:
    

    ~\Documents\anaconda\lib\site-packages\joblib\parallel.py in __call__(self, iterable)
       1002             # remaining jobs.
       1003             self._iterating = False
    -> 1004             if self.dispatch_one_batch(iterator):
       1005                 self._iterating = self._original_iterator is not None
       1006 
    

    ~\Documents\anaconda\lib\site-packages\joblib\parallel.py in dispatch_one_batch(self, iterator)
        833                 return False
        834             else:
    --> 835                 self._dispatch(tasks)
        836                 return True
        837 
    

    ~\Documents\anaconda\lib\site-packages\joblib\parallel.py in _dispatch(self, batch)
        752         with self._lock:
        753             job_idx = len(self._jobs)
    --> 754             job = self._backend.apply_async(batch, callback=cb)
        755             # A job can complete so quickly than its callback is
        756             # called before we get here, causing self._jobs to
    

    ~\Documents\anaconda\lib\site-packages\joblib\_parallel_backends.py in apply_async(self, func, callback)
        207     def apply_async(self, func, callback=None):
        208         """Schedule a func to be run"""
    --> 209         result = ImmediateResult(func)
        210         if callback:
        211             callback(result)
    

    ~\Documents\anaconda\lib\site-packages\joblib\_parallel_backends.py in __init__(self, batch)
        588         # Don't delay the application, to avoid keeping the input
        589         # arguments in memory
    --> 590         self.results = batch()
        591 
        592     def get(self):
    

    ~\Documents\anaconda\lib\site-packages\joblib\parallel.py in __call__(self)
        254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
        255             return [func(*args, **kwargs)
    --> 256                     for func, args, kwargs in self.items]
        257 
        258     def __len__(self):
    

    ~\Documents\anaconda\lib\site-packages\joblib\parallel.py in <listcomp>(.0)
        254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
        255             return [func(*args, **kwargs)
    --> 256                     for func, args, kwargs in self.items]
        257 
        258     def __len__(self):
    

    ~\Documents\anaconda\lib\site-packages\sklearn\neighbors\_base.py in _tree_query_parallel_helper(tree, *args, **kwargs)
        488     under PyPy.
        489     """
    --> 490     return tree.query(*args, **kwargs)
        491 
        492 
    

    KeyboardInterrupt: 

----

The KNN fitting is very bad and achives poor results.

== Final model


+*In[35]:*+
[source, ipython3]
----
# As the lgbm achieves the best results, we shall use it alone after trained on the entire train and cv dataets

x_full = pd.concat([x_train,x_cv])
y_full = pd.concat([y_train,y_cv])


lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=800, max_depth=13,
                                verbosity=1, learning_rate=0.02, n_estimators=1000,
                                subsample=0.8, colsample_bytree=0.8, reg_alpha=0.3)
lgb_model.fit(x_full, np.ravel(y_full))

y_predTrain = lgb_model.predict(x_full)
accTrain = accuracy_score(y_full, y_predTrain)
print("Accuarcy on train set is: " + str(accTrain))

y_pred_lgbm = lgb_model.predict(x_test)
acc = accuracy_score(y_test, y_pred_lgbm)
print("Accuarcy is: " + str(acc))

confusion = confusion_matrix(y_test, y_pred_lgbm)
df_cm = pd.DataFrame(confusion, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (5,5))
sns.set(font_scale=1.4)#for label size
sns.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16}, fmt='g')# font size
sns.despine()
----


+*Out[35]:*+
----
Accuarcy on train set is: 0.6892153125
Accuarcy is: 0.66544875

![png](output_64_1.png)
----
